{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ea50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장토큰화  sent_tokenize\n",
    "# 기본로직\n",
    "    # 마침표,느낌표, 물음표를 문장 끝 후보로 인식\n",
    "    # 약어 패턴 학습(Dr, Mr ,U.S.A 등)\n",
    "    # 대문자로 시작하는지\n",
    "    # 통계적 모델을 사용해 진짜 문장 경계인지 판단\n",
    "# 다국어\n",
    "# 약어와 실제 문장끝을 구분하는 기계학습 모델 내장\n",
    "\n",
    "# 단어 토큰화 word_tokenization\n",
    "    # 규칙기반\n",
    "    # 공백기준 단어분리\n",
    "    # 구두점을 별도 토큰으로 분리\n",
    "    # 축약형 처리 it's  it, s\n",
    "    # 소유격 처리 \"Let's\"  Let s\n",
    "\n",
    "    #구두점기반  WordPunctTokenizer\n",
    "    # 모든 구두점을 분리\n",
    "    # It's  It, ', s\n",
    "\n",
    "    # 정규표현식  RegexpTokenizer\n",
    "\n",
    "# 노이즈와 불용어 제거 \n",
    "    # set자료구조 : 중복제거\n",
    "    # List Comprehension : 필터링\n",
    "    # NLTK 불용어사전\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66b9df22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channel Terms of Service accepted\n",
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e97aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "464bf6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca90203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " \"It's good to see you.\",\n",
       " \"Let's start our text mining class\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello everyone. It's good to see you. Let's start our text mining class\"\n",
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6a954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 학습을 시작해 볼까요?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_kor = '안녕하세요, 여러분. 만나서 반갑습니다. 이제 학습을 시작해 볼까요?'\n",
    "sent_tokenize(sentence_kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61df720e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '학습을', '시작해', '볼까요', '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(sentence_kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09be058f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " '.',\n",
       " 'It',\n",
       " \"'\",\n",
       " 's',\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'start',\n",
       " 'our',\n",
       " 'text',\n",
       " 'mining',\n",
       " 'class']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "WordPunctTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d869d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규식토큰화\n",
    "import re\n",
    "re.findall(\"[abc]\", \"how ard you, boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680acc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sorry', 'go', 'movie', 'yesterday']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노이즈와 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "english_stops = stopwords.words('english')\n",
    "test1 = \"Sorry, I couldn't go to movie yesterday\"\n",
    "# tokens = word_tokenize(test1)\n",
    "tokenizer = RegexpTokenizer(\"[\\\\w']+\")\n",
    "tokens = tokenizer.tokenize(test1.lower())\n",
    "\n",
    "[token for token in tokens if token not in english_stops  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc42ec9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegexpTokenizer(pattern='[\\\\w]', gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 소문자 a~z로 이루어진 문자열에서 4글자이상\n",
    "RegexpTokenizer(\"[a-z]{4,}\")\n",
    "\n",
    "RegexpTokenizer(\"[\\\\w']{3,}\")  # 3글자 이상\n",
    "\n",
    "\n",
    "RegexpTokenizer(\"[\\\\w]\")  # 어포스트로피를 패턴에서 제외   can't   can t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae7522a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cook', 'cookeri', 'cookbook')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어간 추출(Stemming)\n",
    "# 줄기 stem   단어에서 불필요한 요소를 제거하고 남는 핵심형태\n",
    "# 단어는 다양한 형태... 복수형 과거형 과 같은 시제변환, 복수형\n",
    "# 단어를 통일\n",
    "    # walk(걷다)   walks walking walked  -->어간 walk\n",
    "    # 먹는다, 사랑해, 책을 \n",
    "    #  먹는다 먹었다... '먹-'으로 묶어서 컴퓨터가 같은 단어로 인식  \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4eb3edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer 규칙기반이라서 완벽하지 못함 --> 속도가 빠름, 의미가 달라질수 도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c806e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cook', 'cookery', 'cookbook')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer  # 더 많은 규칙이 적용.. 과도한 축약 위험\n",
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f49b5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 추출  Lemmatization\n",
    "# Lemma  단어의 사전 기본형\n",
    "# 단어의 변형(시제,복수,비교급) 제거하고 사전(headword)에 나오는 정확한 원형으로 바꾸는 과정\n",
    "# 어간처럼 단어줄기가 아니라 , 맥락과 품사를 고려한 올바른 형태\n",
    "# better(더 좋은) ->표제어  good(좋은)\n",
    "# 먹었다 --> 먹다(동사 원형)\n",
    "# 알고리즘 : 형태소 분석기(konlpy) 를 사용해 품사(명사,동사)를 보고 정확히 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b02b03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요목적\n",
    "    # 어간추출처럼 대충 줄이지 않고 맥락에 맞는 정확한 단어로 만들어서 NLP 품질 향상\n",
    "    # 단점  사전에 의존해서 언어/맥락 제한\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699ec5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print( lemmatizer.lemmatize('cooking'))  # 기본이 명사로 인식\n",
    "print(lemmatizer.lemmatize('cooking',pos='v'))  # 품사를 동사(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "361b177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n  noun(명사)  -->기본\n",
    "# v  verb(동사)\n",
    "# a  adjective(형용사)\n",
    "# r  averb (부사)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66b141b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better', pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4a9a5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hello', 'NN'),\n",
       " ('everyone', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('good', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('see', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Let', 'VB'),\n",
       " (\"'s\", 'POS'),\n",
       " ('start', 'VB'),\n",
       " ('out', 'RP'),\n",
       " ('text', 'NN'),\n",
       " ('mining', 'NN'),\n",
       " ('calss', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 태깅\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "tokens = \"hello everyone. It's good to see you. Let's start out text mining calss!\"\n",
    "tokens = word_tokenize(tokens)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd5b7635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "# 품사 태그 정보 확인\n",
    "# nltk.download('tagsets_json')\n",
    "nltk.help.upenn_tagset('NN')  # 명사 동사 형용사   NN VB JJ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac8b8e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'calss']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특정 품사 추출   명사 동사 형용사\n",
    "tag_lists = ['NN','VB','JJ']\n",
    "[word for word,tag in nltk.pos_tag(tokens) if tag in tag_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e46289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK는 영어기반  한국어의 조사분리 불가능, 어미변화처리 불가능\n",
    "# KoNlpy Okt 사용 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb991d",
   "metadata": {},
   "source": [
    "setx JAVA_HOME \"C:\\Users\\playdata2\\Downloads\\openjdk-25.0.1_windows-x64_bin\\jdk-25.0.1\"\n",
    "\n",
    "setx PATH \"%PATH%;%JAVA_HOME%\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5da7e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: JPype1 in c:\\users\\playdata2\\miniconda3\\envs\\llm\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\playdata2\\appdata\\roaming\\python\\python313\\site-packages (from JPype1) (25.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install JPype1\n",
    "\n",
    "# 컴퓨터 재부팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8e06df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['절망의', '반대가', '희망은', '아니다', '.', '어두운', '밤하늘에', '별이', '빛나듯', '희망은', '절망', '속에', '싹트는', '거지', '만약에', '우리가', '희망함이', '적다면', '그', '누가', '세상을', '비출어줄까', '.', '정희성', ',', '희망', '공부']\n",
      "[('절망의', 'JJ'), ('반대가', 'NNP'), ('희망은', 'NNP'), ('아니다', 'NNP'), ('.', '.'), ('어두운', 'VB'), ('밤하늘에', 'JJ'), ('별이', 'NNP'), ('빛나듯', 'NNP'), ('희망은', 'NNP'), ('절망', 'NNP'), ('속에', 'NNP'), ('싹트는', 'NNP'), ('거지', 'NNP'), ('만약에', 'NNP'), ('우리가', 'NNP'), ('희망함이', 'NNP'), ('적다면', 'NNP'), ('그', 'NNP'), ('누가', 'NNP'), ('세상을', 'NNP'), ('비출어줄까', 'NNP'), ('.', '.'), ('정희성', 'NN'), (',', ','), ('희망', 'NNP'), ('공부', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "sentence = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망함이 적다면\n",
    "그 누가 세상을 비출어줄까.\n",
    "정희성, 희망 공부'''\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccac1ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 : ['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나듯', '\\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다면', '\\n', '그', '누가', '세상', '을', '비출어줄까', '.', '\\n', '정희성', ',', '희망', '공부']\n",
      "명사 : ['절망', '반대', '희망', '어', '두운', '밤하늘', '별', '희망', '절망', '속', '거지', '만약', '우리', '희망', '함', '그', '누가', '세상', '정희성', '희망', '공부']\n",
      "품사태깅 : [('절망', 'Noun'), ('의', 'Josa'), ('반대', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('은', 'Josa'), ('아니다', 'Adjective'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('어', 'Noun'), ('두운', 'Noun'), ('밤하늘', 'Noun'), ('에', 'Josa'), ('별', 'Noun'), ('이', 'Josa'), ('빛나듯', 'Verb'), ('\\n', 'Foreign'), ('희망', 'Noun'), ('은', 'Josa'), ('절망', 'Noun'), ('속', 'Noun'), ('에', 'Josa'), ('싹트는', 'Verb'), ('거지', 'Noun'), ('\\n', 'Foreign'), ('만약', 'Noun'), ('에', 'Josa'), ('우리', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('함', 'Noun'), ('이', 'Josa'), ('적다면', 'Verb'), ('\\n', 'Foreign'), ('그', 'Noun'), ('누가', 'Noun'), ('세상', 'Noun'), ('을', 'Josa'), ('비출어줄까', 'Verb'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('정희성', 'Noun'), (',', 'Punctuation'), ('희망', 'Noun'), ('공부', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "# NNP 고유명사... 대부분 고유명사로 잘못 인식\n",
    "from konlpy.tag import Okt\n",
    "t = Okt()\n",
    "print(f'형태소 : {t.morphs(sentence)}')\n",
    "print(f'명사 : {t.nouns(sentence)}')\n",
    "print(f'품사태깅 : {t.pos(sentence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff00bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프와 워드클라우드\n",
    "\n",
    "# 1. 데이터 로딩\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg  # 쿠텐버그 말뭉치중에서 이상한나라의 엘리스 텍스트 로드\n",
    "gutenberg.fileids()\n",
    "doc_alice =  gutenberg.open('carroll-alice.txt').read()\n",
    "print(doc_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2cc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33535\n",
      "21616\n"
     ]
    }
   ],
   "source": [
    "# 2. 토큰화 및 전처리\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens_alice = word_tokenize(doc_alice)\n",
    "print(len(tokens_alice))\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\\\w']{3,}\")\n",
    "reg_alice = tokenizer.tokenize(doc_alice.lower())\n",
    "print(len(reg_alice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a4e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12871"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "english_stops =  set(stopwords.words('english'))\n",
    "result_alice = [ word for word in reg_alice if word not in english_stops  ]\n",
    "len(result_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2dcff1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 품사 태깅 및 필터링 (명사 동사 형용사만)\n",
    "my_tag_set = ['NN','VB','VBD','JJ']\n",
    "my_word = [word for word, tag in nltk.pos_tag(reg_alice) if tag in my_tag_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7178f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "sorted_word_count = dict( Counter(my_word) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9bf656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
