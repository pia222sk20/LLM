{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29c52ed",
   "metadata": {},
   "source": [
    "- TF-IDF : 텍스트 벡터화\n",
    "- PCA : 차원 축소\n",
    "- LSA : 잠재 의미 분석\n",
    "- t-SNE : 2D시각화\n",
    "- 로지스틱회귀\n",
    "- 토큰화 & 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41bdac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "# TF-IDF 행렬에 대해서 SVD 를 적용\n",
    "# 단어와 문서 간의 숨겨진 의미 관계를 찾음\n",
    "# PCA 차이:\n",
    "    # PCA : 데이터 자체 분산 최대화\n",
    "    # LSA : 문서-단어형태의 의미 구조 파악\n",
    "# \"은행\"\n",
    "    # \"돈\"  \"계좌\"     주변에 등장\n",
    "    # \"나무\" \"냄세\" \"먹는다\" 주변에 등장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dcdd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE : 고차원 데이터를 2D/3D로 변환 - 시각화 전용(분석에는 부적합), 계산이 오래걸림\n",
    "# PCA vs t-SNE  \n",
    "# PCA : 속도가 빠름, 전역 구조 보존\n",
    "# t-SNE : 느림, 국소(지역) 군집 명확"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4fc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋\n",
    "from sklearn.datasets import load_files\n",
    "train_path = r'C:\\LLM\\ch05\\20newsbydate\\20news-bydate-train'\n",
    "test_path = r'C:\\LLM\\ch05\\20newsbydate\\20news-bydate-test'\n",
    "newsgroups_train = load_files(train_path,encoding='latin1')\n",
    "newsgroups_test = load_files(test_path,encoding='latin1')\n",
    "categories =  ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 헤더 제거\n",
    "    text = re.sub(r'^From:.*\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Subject:.*\\n', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 풋터 제거\n",
    "    text = re.sub(r'\\n--\\n.*$', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # 인용문 제거\n",
    "    text = re.sub(r'(^|\\n)[>|:].*', '', text)\n",
    "\n",
    "    return text\n",
    "# 카테고리 제거\n",
    "def filter_categories(dataset, categories):\n",
    "    target_names = dataset.target_names\n",
    "    selected_idx = [ target_names.index(c) for c in categories  ]\n",
    "    #필터링\n",
    "    data_filtered, target_filtered = [], []\n",
    "    for text,label in zip(dataset.data, dataset.target):\n",
    "        if label in selected_idx:\n",
    "            new_label = selected_idx.index(label)  # 라벨 재 정렬\n",
    "            data_filtered.append(text) ; target_filtered.append( new_label  )\n",
    "    return data_filtered,target_filtered,categories\n",
    "train_data, train_target, target_names = filter_categories(newsgroups_train,categories)\n",
    "test_data, test_target, _ = filter_categories(newsgroups_test,categories)\n",
    "\n",
    "x_train = [ clean_text(t) for t in train_data]\n",
    "x_test = [ clean_text(t) for t in test_data]\n",
    "y_train = train_target\n",
    "y_test = test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3438eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본분류------------\n",
      "학습정확도 : 0.9705014749262537\n",
      "테스트정확도 : 0.8174427198817442\n",
      "주성분분석\n",
      "원본 차원 : 2000\n",
      "축소후 차원 : 100\n",
      "설명된 분산 : 0.3282151895915271\n",
      "누적 분산 : [0.01211923 0.02229888 0.03189694 0.04138118 0.0493389  0.05676164\n",
      " 0.0635188  0.0701598  0.0765997  0.0826845 ]\n",
      "주성분 분석 분류------------\n",
      "학습정확도 : 0.9011799410029498\n",
      "테스트정확도 : 0.7923133776792314\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 전처리 ( 소문자 + 토큰화(3글자이상) + 불용어제거(stopwords) + 어간추출(stemming))  --> 영어\n",
    "# 파이프라인 \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer  # 같은 의미의 다른형태 단어를 통일\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "regtok = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "english_stops = set(stopwords.words('english'))\n",
    "# 커스텀 토크나이져\n",
    "def tokenizer(text):\n",
    "    tokens = regtok.tokenize(text)\n",
    "    words = [word for word in tokens if word not in english_stops]\n",
    "    features = list(map(lambda x : PorterStemmer().stem(x), words))\n",
    "    return features\n",
    "# TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer,max_features=2000,min_df=2,max_df=0.5)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)\n",
    "# 분류모델\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf = LogisticRegression(max_iter=200,random_state=42)\n",
    "lr_clf.fit(x_train_tfidf,y_train)\n",
    "\n",
    "print('기본분류------------')\n",
    "print(f'학습정확도 : {lr_clf.score(x_train_tfidf,y_train)}')\n",
    "print(f'테스트정확도 : {lr_clf.score(x_test_tfidf,y_test)}')\n",
    "\n",
    "print('주성분분석')\n",
    "# 데이터의 분산이 가장 큰 방향\n",
    "# 선형번환만 가능\n",
    "# 모든 데이터의 특성을 평등하게 고려\n",
    "# 용도 : 시각화, 속도 개선\n",
    "# 2000차원 - >100차원축소\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100,random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_tfidf.toarray())\n",
    "x_test_pca = pca.transform(x_test_tfidf.toarray())\n",
    "\n",
    "import numpy as np\n",
    "cumsum_var =  np.cumsum( pca.explained_variance_ratio_)\n",
    "print(f'원본 차원 : {x_train_tfidf.shape[1]}')\n",
    "print(f'축소후 차원 : {x_train_pca.shape[1]}')\n",
    "print(f'설명된 분산 : {pca.explained_variance_.sum()}')\n",
    "print(f'누적 분산 : {cumsum_var[:10]}')\n",
    "\n",
    "# pca 후 분류\n",
    "lr_clf_pca = LogisticRegression(max_iter=200,random_state=42)\n",
    "lr_clf_pca.fit(x_train_pca,y_train)\n",
    "\n",
    "print('주성분 분석 분류------------')\n",
    "print(f'학습정확도 : {lr_clf_pca.score(x_train_pca,y_train)}')\n",
    "print(f'테스트정확도 : {lr_clf_pca.score(x_test_pca,y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
