{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8233d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 영화 리뷰 수: 2000\n",
      "카테고리: ['neg', 'pos']\n",
      "부정 리뷰: 1000개\n",
      "긍정 리뷰: 1000개\n",
      "\n",
      "첫 번째 리뷰 ID: neg/cv000_29416.txt\n",
      "원문 일부:\n",
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "w\n",
      "\n",
      "문장 토큰화 (첫 2개):\n",
      "  1: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.']\n",
      "  2: ['they', 'get', 'into', 'an', 'accident', '.']\n",
      "\n",
      "단어 토큰화 (첫 20개): ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1단계: 필요한 데이터 다운로드\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2단계: 데이터 탐색\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# 데이터셋 크기 파악\n",
    "print(f\"전체 영화 리뷰 수: {len(movie_reviews.fileids())}\")\n",
    "print(f\"카테고리: {movie_reviews.categories()}\")  # ['neg', 'pos']\n",
    "print(f\"부정 리뷰: {len(movie_reviews.fileids(categories='neg'))}개\")\n",
    "print(f\"긍정 리뷰: {len(movie_reviews.fileids(categories='pos'))}개\")\n",
    "\n",
    "# 3단계: 첫 번째 리뷰 살펴보기\n",
    "first_review_id = movie_reviews.fileids()[0]\n",
    "first_review = movie_reviews.raw(first_review_id)\n",
    "print(f\"\\n첫 번째 리뷰 ID: {first_review_id}\")\n",
    "print(f\"원문 일부:\\n{first_review[:200]}\")\n",
    "\n",
    "# 4단계: 토큰화 결과 확인\n",
    "sentences = movie_reviews.sents(first_review_id)  # 문장 단위 토큰화\n",
    "words = movie_reviews.words(first_review_id)      # 단어 단위 토큰화\n",
    "\n",
    "print(f\"\\n문장 토큰화 (첫 2개):\")\n",
    "for i, sent in enumerate(sentences[:2]):\n",
    "    print(f\"  {i+1}: {sent}\")\n",
    "\n",
    "print(f\"\\n단어 토큰화 (첫 20개): {words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc06b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpTokenizer: 정규표현식으로 정확한 토큰화\n",
    "# stopwords : 문법적 기능을 제거하고 단어에 집중\n",
    "# 상위 N개 단어 선택 : 메모리효율성과 노이즈 제거의 균형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0db137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서 수: 2000\n",
      "첫 문서의 단어 수: 879\n",
      "첫 문서의 첫 50개 단어:\n",
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch']\n",
      "\n",
      "상위 10개 빈도 단어:\n",
      "  1. ',': 77717회\n",
      "  2. 'the': 76529회\n",
      "  3. '.': 65876회\n",
      "  4. 'a': 38106회\n",
      "  5. 'and': 35576회\n",
      "  6. 'of': 34123회\n",
      "  7. 'to': 31937회\n",
      "  8. ''': 30585회\n",
      "  9. 'is': 25195회\n",
      "  10. 'in': 21822회\n",
      "\n",
      "전체 서로 다른 단어 수: 43011\n",
      "\n",
      "처리 후 상위 10개 단어:\n",
      "  1. 'film': 8935회\n",
      "  2. 'one': 5791회\n",
      "  3. 'movie': 5538회\n",
      "  4. 'like': 3690회\n",
      "  5. 'even': 2564회\n",
      "  6. 'time': 2409회\n",
      "  7. 'good': 2407회\n",
      "  8. 'story': 2136회\n",
      "  9. 'would': 2084회\n",
      "  10. 'much': 2049회\n",
      "\n",
      "특성으로 선택된 단어 수: 1000\n",
      "특성 예시: ['film', 'one', 'movie', 'like', 'even', 'time', 'good', 'story', 'would', 'much', 'also', 'get', 'character', 'two', 'well', 'first', 'characters', 'see', 'way', 'make']\n"
     ]
    }
   ],
   "source": [
    "# BOW - 수동으로 벡터 생성\n",
    "# 1단계: 모든 문서를 단어 리스트로 변환\n",
    "documents = [list(movie_reviews.words(fileid)) \n",
    "             for fileid in movie_reviews.fileids()]\n",
    "\n",
    "print(f\"전체 문서 수: {len(documents)}\")\n",
    "print(f\"첫 문서의 단어 수: {len(documents[0])}\")\n",
    "print(f\"첫 문서의 첫 50개 단어:\\n{documents[0][:50]}\")\n",
    "\n",
    "# 2단계: 전체 단어 빈도 계산 (불용어 제외 전)\n",
    "word_count = {}\n",
    "for doc in documents:\n",
    "    for word in doc:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "# 상위 10개 빈도 단어 확인\n",
    "sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n상위 10개 빈도 단어:\")\n",
    "for i, (word, count) in enumerate(sorted_words[:10], 1):\n",
    "    print(f\"  {i}. '{word}': {count}회\")\n",
    "\n",
    "# 3단계: 불용어 제거 후 처리\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 정규표현식으로 3글자 이상의 단어만 추출\n",
    "tokenizer = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "# 영어 불용어 로드\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# 모든 리뷰를 토큰화하고 불용어 제거\n",
    "processed_documents = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    raw_text = movie_reviews.raw(fileid)\n",
    "    tokens = [token for token in tokenizer.tokenize(raw_text) \n",
    "              if token not in english_stops]\n",
    "    processed_documents.append(tokens)\n",
    "\n",
    "# 처리 후 단어 빈도 재계산\n",
    "word_count_processed = {}\n",
    "for doc in processed_documents:\n",
    "    for word in doc:\n",
    "        word_count_processed[word] = word_count_processed.get(word, 0) + 1\n",
    "\n",
    "sorted_processed = sorted(word_count_processed.items(), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n전체 서로 다른 단어 수: {len(sorted_processed)}\")\n",
    "print(\"\\n처리 후 상위 10개 단어:\")\n",
    "for i, (word, count) in enumerate(sorted_processed[:10], 1):\n",
    "    print(f\"  {i}. '{word}': {count}회\")\n",
    "\n",
    "# 4단계: 특성 선택 (상위 1000개 단어)\n",
    "word_features = [word for word, count in sorted_processed[:1000]]\n",
    "print(f\"\\n특성으로 선택된 단어 수: {len(word_features)}\")\n",
    "print(f\"특성 예시: {word_features[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b3dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 130 262 278 391 "
     ]
    }
   ],
   "source": [
    "# processed_documents[0]  # 문장을 토큰화(3개의 연속된 문장, 불용어제거)\n",
    "for doc in processed_documents[:5]:\n",
    "    print(len(doc), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e53892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', 'two', 'teen']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_documents[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3dd948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 단어 리스트: ['one', 'two', 'teen', 'couples', 'solo']\n",
      "테스트 문서: ['two', 'two', 'couples']\n",
      "결과 벡터: [0, 2, 0, 1, 0]\n",
      "→ 'two'가 2번, 'couples'가 1번, 나머지는 0\n",
      "\n",
      "생성된 특성 벡터 수: 2000\n",
      "각 벡터의 차원: 1000\n",
      "\n",
      "첫 문서 벡터 (처음 20개):\n",
      "  'film': 5\n",
      "  'one': 3\n",
      "  'movie': 6\n",
      "  'like': 3\n",
      "  'even': 3\n",
      "  'time': 0\n",
      "  'good': 2\n",
      "  'story': 0\n",
      "  'would': 1\n",
      "  'much': 0\n",
      "  'also': 1\n",
      "  'get': 3\n",
      "  'character': 1\n",
      "  'two': 2\n",
      "  'well': 1\n",
      "  'first': 0\n",
      "  'characters': 1\n",
      "  'see': 2\n",
      "  'way': 3\n",
      "  'make': 5\n"
     ]
    }
   ],
   "source": [
    "# 각 문서의 고정된 길이의 벡터로 변환(모든 문서가 같은 차원 )\n",
    "# 기계학습 알고리즘의 입력 형식으로 변환\n",
    "def document_features(document, word_features):\n",
    "    \"\"\"\n",
    "    문서를 특성 벡터로 변환\n",
    "    \n",
    "    Args:\n",
    "        document: 토큰화된 단어 리스트\n",
    "        word_features: 특성으로 사용할 단어 리스트\n",
    "    \n",
    "    Returns:\n",
    "        document의 각 특성에 대한 빈도 리스트\n",
    "    \"\"\"\n",
    "    # 문서 내 단어 빈도 계산\n",
    "    word_count = {}\n",
    "    for word in document:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "    \n",
    "    # 특성 벡터 생성\n",
    "    features = []\n",
    "    for word in word_features:\n",
    "        # 특성 단어가 문서에 없으면 0\n",
    "        features.append(word_count.get(word, 0))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 테스트 실행\n",
    "test_features = ['one', 'two', 'teen', 'couples', 'solo']\n",
    "test_doc = ['two', 'two', 'couples']\n",
    "result = document_features(test_doc, test_features)\n",
    "\n",
    "print(\"테스트 단어 리스트:\", test_features)\n",
    "print(\"테스트 문서:\", test_doc)\n",
    "print(\"결과 벡터:\", result)\n",
    "print(\"→ 'two'가 2번, 'couples'가 1번, 나머지는 0\")\n",
    "\n",
    "# 모든 문서에 대해 특성 벡터 생성\n",
    "feature_sets = [document_features(doc, word_features) \n",
    "                 for doc in processed_documents]\n",
    "\n",
    "print(f\"\\n생성된 특성 벡터 수: {len(feature_sets)}\")\n",
    "print(f\"각 벡터의 차원: {len(feature_sets[0])}\")\n",
    "print(f\"\\n첫 문서 벡터 (처음 20개):\")\n",
    "for i, (word, count) in enumerate(zip(word_features[:20], feature_sets[0][:20])):\n",
    "    print(f\"  '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5889115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['film', 'one', 'movie'], [5, 3, 6], 1000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features[:3],  feature_sets[0][:3],  len(word_features), len(feature_sets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64beacb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성수 : ['film' 'one' 'movie' 'like' 'even']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 데이터 준비\n",
    "reivews = [  movie_reviews.raw(fileid) for fileid in movie_reviews.fileids() ]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(vocabulary=word_features)\n",
    "reviews_cv = cv.fit_transform(reivews)\n",
    "print(f'특성수 : {cv.get_feature_names_out()[:5]}')\n",
    "reviews_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40a9006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f733e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mdaum_movie_review.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m df.head(\u001b[32m2\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('daum_movie_review.csv')\n",
    "df.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
