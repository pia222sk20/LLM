{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8233d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 영화 리뷰 수: 2000\n",
      "카테고리: ['neg', 'pos']\n",
      "부정 리뷰: 1000개\n",
      "긍정 리뷰: 1000개\n",
      "\n",
      "첫 번째 리뷰 ID: neg/cv000_29416.txt\n",
      "원문 일부:\n",
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "w\n",
      "\n",
      "문장 토큰화 (첫 2개):\n",
      "  1: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.']\n",
      "  2: ['they', 'get', 'into', 'an', 'accident', '.']\n",
      "\n",
      "단어 토큰화 (첫 20개): ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1단계: 필요한 데이터 다운로드\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2단계: 데이터 탐색\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# 데이터셋 크기 파악\n",
    "print(f\"전체 영화 리뷰 수: {len(movie_reviews.fileids())}\")\n",
    "print(f\"카테고리: {movie_reviews.categories()}\")  # ['neg', 'pos']\n",
    "print(f\"부정 리뷰: {len(movie_reviews.fileids(categories='neg'))}개\")\n",
    "print(f\"긍정 리뷰: {len(movie_reviews.fileids(categories='pos'))}개\")\n",
    "\n",
    "# 3단계: 첫 번째 리뷰 살펴보기\n",
    "first_review_id = movie_reviews.fileids()[0]\n",
    "first_review = movie_reviews.raw(first_review_id)\n",
    "print(f\"\\n첫 번째 리뷰 ID: {first_review_id}\")\n",
    "print(f\"원문 일부:\\n{first_review[:200]}\")\n",
    "\n",
    "# 4단계: 토큰화 결과 확인\n",
    "sentences = movie_reviews.sents(first_review_id)  # 문장 단위 토큰화\n",
    "words = movie_reviews.words(first_review_id)      # 단어 단위 토큰화\n",
    "\n",
    "print(f\"\\n문장 토큰화 (첫 2개):\")\n",
    "for i, sent in enumerate(sentences[:2]):\n",
    "    print(f\"  {i+1}: {sent}\")\n",
    "\n",
    "print(f\"\\n단어 토큰화 (첫 20개): {words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc06b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpTokenizer: 정규표현식으로 정확한 토큰화\n",
    "# stopwords : 문법적 기능을 제거하고 단어에 집중\n",
    "# 상위 N개 단어 선택 : 메모리효율성과 노이즈 제거의 균형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0db137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서 수: 2000\n",
      "첫 문서의 단어 수: 879\n",
      "첫 문서의 첫 50개 단어:\n",
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch']\n",
      "\n",
      "상위 10개 빈도 단어:\n",
      "  1. ',': 77717회\n",
      "  2. 'the': 76529회\n",
      "  3. '.': 65876회\n",
      "  4. 'a': 38106회\n",
      "  5. 'and': 35576회\n",
      "  6. 'of': 34123회\n",
      "  7. 'to': 31937회\n",
      "  8. ''': 30585회\n",
      "  9. 'is': 25195회\n",
      "  10. 'in': 21822회\n",
      "\n",
      "전체 서로 다른 단어 수: 43011\n",
      "\n",
      "처리 후 상위 10개 단어:\n",
      "  1. 'film': 8935회\n",
      "  2. 'one': 5791회\n",
      "  3. 'movie': 5538회\n",
      "  4. 'like': 3690회\n",
      "  5. 'even': 2564회\n",
      "  6. 'time': 2409회\n",
      "  7. 'good': 2407회\n",
      "  8. 'story': 2136회\n",
      "  9. 'would': 2084회\n",
      "  10. 'much': 2049회\n",
      "\n",
      "특성으로 선택된 단어 수: 1000\n",
      "특성 예시: ['film', 'one', 'movie', 'like', 'even', 'time', 'good', 'story', 'would', 'much', 'also', 'get', 'character', 'two', 'well', 'first', 'characters', 'see', 'way', 'make']\n"
     ]
    }
   ],
   "source": [
    "# BOW - 수동으로 벡터 생성\n",
    "# 1단계: 모든 문서를 단어 리스트로 변환\n",
    "documents = [list(movie_reviews.words(fileid)) \n",
    "             for fileid in movie_reviews.fileids()]\n",
    "\n",
    "print(f\"전체 문서 수: {len(documents)}\")\n",
    "print(f\"첫 문서의 단어 수: {len(documents[0])}\")\n",
    "print(f\"첫 문서의 첫 50개 단어:\\n{documents[0][:50]}\")\n",
    "\n",
    "# 2단계: 전체 단어 빈도 계산 (불용어 제외 전)\n",
    "word_count = {}\n",
    "for doc in documents:\n",
    "    for word in doc:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "# 상위 10개 빈도 단어 확인\n",
    "sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n상위 10개 빈도 단어:\")\n",
    "for i, (word, count) in enumerate(sorted_words[:10], 1):\n",
    "    print(f\"  {i}. '{word}': {count}회\")\n",
    "\n",
    "# 3단계: 불용어 제거 후 처리\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 정규표현식으로 3글자 이상의 단어만 추출\n",
    "tokenizer = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "# 영어 불용어 로드\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# 모든 리뷰를 토큰화하고 불용어 제거\n",
    "processed_documents = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    raw_text = movie_reviews.raw(fileid)\n",
    "    tokens = [token for token in tokenizer.tokenize(raw_text) \n",
    "              if token not in english_stops]\n",
    "    processed_documents.append(tokens)\n",
    "\n",
    "# 처리 후 단어 빈도 재계산\n",
    "word_count_processed = {}\n",
    "for doc in processed_documents:\n",
    "    for word in doc:\n",
    "        word_count_processed[word] = word_count_processed.get(word, 0) + 1\n",
    "\n",
    "sorted_processed = sorted(word_count_processed.items(), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n전체 서로 다른 단어 수: {len(sorted_processed)}\")\n",
    "print(\"\\n처리 후 상위 10개 단어:\")\n",
    "for i, (word, count) in enumerate(sorted_processed[:10], 1):\n",
    "    print(f\"  {i}. '{word}': {count}회\")\n",
    "\n",
    "# 4단계: 특성 선택 (상위 1000개 단어)\n",
    "word_features = [word for word, count in sorted_processed[:1000]]\n",
    "print(f\"\\n특성으로 선택된 단어 수: {len(word_features)}\")\n",
    "print(f\"특성 예시: {word_features[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b3dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 130 262 278 391 "
     ]
    }
   ],
   "source": [
    "# processed_documents[0]  # 문장을 토큰화(3개의 연속된 문장, 불용어제거)\n",
    "for doc in processed_documents[:5]:\n",
    "    print(len(doc), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e53892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', 'two', 'teen']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_documents[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3dd948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 단어 리스트: ['one', 'two', 'teen', 'couples', 'solo']\n",
      "테스트 문서: ['two', 'two', 'couples']\n",
      "결과 벡터: [0, 2, 0, 1, 0]\n",
      "→ 'two'가 2번, 'couples'가 1번, 나머지는 0\n",
      "\n",
      "생성된 특성 벡터 수: 2000\n",
      "각 벡터의 차원: 1000\n",
      "\n",
      "첫 문서 벡터 (처음 20개):\n",
      "  'film': 5\n",
      "  'one': 3\n",
      "  'movie': 6\n",
      "  'like': 3\n",
      "  'even': 3\n",
      "  'time': 0\n",
      "  'good': 2\n",
      "  'story': 0\n",
      "  'would': 1\n",
      "  'much': 0\n",
      "  'also': 1\n",
      "  'get': 3\n",
      "  'character': 1\n",
      "  'two': 2\n",
      "  'well': 1\n",
      "  'first': 0\n",
      "  'characters': 1\n",
      "  'see': 2\n",
      "  'way': 3\n",
      "  'make': 5\n"
     ]
    }
   ],
   "source": [
    "# 각 문서의 고정된 길이의 벡터로 변환(모든 문서가 같은 차원 )\n",
    "# 기계학습 알고리즘의 입력 형식으로 변환\n",
    "def document_features(document, word_features):\n",
    "    \"\"\"\n",
    "    문서를 특성 벡터로 변환\n",
    "    \n",
    "    Args:\n",
    "        document: 토큰화된 단어 리스트\n",
    "        word_features: 특성으로 사용할 단어 리스트\n",
    "    \n",
    "    Returns:\n",
    "        document의 각 특성에 대한 빈도 리스트\n",
    "    \"\"\"\n",
    "    # 문서 내 단어 빈도 계산\n",
    "    word_count = {}\n",
    "    for word in document:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "    \n",
    "    # 특성 벡터 생성\n",
    "    features = []\n",
    "    for word in word_features:\n",
    "        # 특성 단어가 문서에 없으면 0\n",
    "        features.append(word_count.get(word, 0))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 테스트 실행\n",
    "test_features = ['one', 'two', 'teen', 'couples', 'solo']\n",
    "test_doc = ['two', 'two', 'couples']\n",
    "result = document_features(test_doc, test_features)\n",
    "\n",
    "print(\"테스트 단어 리스트:\", test_features)\n",
    "print(\"테스트 문서:\", test_doc)\n",
    "print(\"결과 벡터:\", result)\n",
    "print(\"→ 'two'가 2번, 'couples'가 1번, 나머지는 0\")\n",
    "\n",
    "# 모든 문서에 대해 특성 벡터 생성\n",
    "feature_sets = [document_features(doc, word_features) \n",
    "                 for doc in processed_documents]\n",
    "\n",
    "print(f\"\\n생성된 특성 벡터 수: {len(feature_sets)}\")\n",
    "print(f\"각 벡터의 차원: {len(feature_sets[0])}\")\n",
    "print(f\"\\n첫 문서 벡터 (처음 20개):\")\n",
    "for i, (word, count) in enumerate(zip(word_features[:20], feature_sets[0][:20])):\n",
    "    print(f\"  '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5889115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['film', 'one', 'movie'], [5, 3, 6], 1000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features[:3],  feature_sets[0][:3],  len(word_features), len(feature_sets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64beacb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성수 : ['film' 'one' 'movie' 'like' 'even' 'time' 'good' 'story' 'would' 'much'\n",
      " 'also' 'get' 'character' 'two' 'well' 'first' 'characters' 'see' 'way'\n",
      " 'make' 'life' 'really' 'films' 'plot' 'little' 'people' 'could' 'bad'\n",
      " 'scene' 'never' 'best' 'man' 'new' 'scenes' 'many' 'know' 'movies'\n",
      " 'director' 'action' 'great' 'another' 'love' 'made' 'big' 'end' 'back'\n",
      " 'something' 'still' 'seems' 'work' 'makes' \"there's\" 'world' 'however'\n",
      " 'every' 'though' 'better' 'real' 'seen' 'enough' 'take' 'around' 'going'\n",
      " 'performance' 'audience' 'role' 'old' 'gets' 'may' 'things' 'think'\n",
      " 'years' 'last' 'actually' 'comedy' 'look' 'funny' 'long' 'almost' 'thing'\n",
      " 'fact' 'nothing' 'say' 'right' 'although' 'played' 'find' \"that's\" 'john'\n",
      " 'come' 'ever' 'since' 'cast' 'script' 'year' 'plays' 'star' 'young'\n",
      " 'comes' 'show' 'part' 'screen' 'original' 'without' 'actors' 'acting'\n",
      " 'three' 'point' 'least' 'lot' 'takes' 'day' 'quite' 'away' 'effects'\n",
      " 'course' 'goes' \"can't\" 'minutes' 'guy' 'interesting' 'far' 'family'\n",
      " 'might' 'high' 'rather' 'anything' 'must' 'place' 'set' 'yet' 'watch'\n",
      " 'making' 'always' 'hard' 'fun' \"film's\" 'seem' 'special' 'bit' 'times'\n",
      " 'wife' 'trying' 'instead' 'give' 'want' 'kind' 'american' 'job' 'sense'\n",
      " 'picture' 'home' 'probably' 'series' 'help' 'half' 'along' 'hollywood'\n",
      " 'pretty' 'becomes' 'everything' 'actor' 'woman' 'sure' 'together' 'men'\n",
      " 'black' 'dialogue' 'become' 'money' 'gives' 'given' 'looking' 'whole'\n",
      " 'watching' 'feel' 'music' 'wants' 'less' 'sex' 'done' 'got' 'horror'\n",
      " 'death' 'perhaps' 'next' 'especially' 'play' 'mind' 'moments' 'everyone'\n",
      " 'father' 'looks' 'city' 'completely' 'reason' 'whose' 'line'\n",
      " 'performances' 'different' 'rest' 'human' 'girl' 'small' 'simply' 'night'\n",
      " 'put' 'james' 'ending' 'couple' 'case' 'several' 'evil' 'left' 'thought'\n",
      " 'dead' 'anyone' 'michael' 'humor' 'shows' 'written' 'entire' 'true'\n",
      " 'lost' 'mother' 'getting' 'turns' 'soon' 'second' 'main' 'stars' 'found'\n",
      " 'use' 'school' 'problem' 'begins' 'friends' 'name' 'called' 'top' 'comic'\n",
      " 'based' 'idea' 'head' 'wrong' 'town' 'either' 'unfortunately' 'later'\n",
      " 'final' 'friend' 'someone' 'hand' 'alien' 'else' 'full' 'david' 'house'\n",
      " 'used' 'tries' 'often' 'group' 'war' 'sequence' 'keep' 'turn' 'playing'\n",
      " 'behind' 'named' 'certainly' 'live' 'believe' 'works' 'relationship'\n",
      " 'hour' 'face' 'style' 'said' 'despite' 'boy' 'finally' 'run' 'shot'\n",
      " 'tell' 'nice' 'maybe' 'book' 'perfect' 'side' 'seeing' 'able' 'finds'\n",
      " 'past' 'camera' 'person' 'including' 'days' 'lives' 'directed' 'running'\n",
      " 'fight' 'supposed' 'matter' 'moment' 'video' 'lines' 'car' 'worth' 'game'\n",
      " 'starts' 'need' 'son' 'entertaining' 'kids' 'start' 'summer' 'short'\n",
      " 'self' 'worst' 'nearly' 'white' 'dark' 'daughter' 'opening' 'try' 'upon'\n",
      " 'kevin' 'care' 'early' 'violence' 'throughout' 'writer' 'example'\n",
      " 'production' 'title' 'exactly' 'beautiful' 'earth' 'let' 'review' 'drama'\n",
      " 'major' 'problems' 'sequences' 'obvious' 'known' 'version' 'robert' 'joe'\n",
      " 'already' 'classic' 'screenplay' 'team' \"who's\" 'close' 'kill' 'others'\n",
      " 'hit' 'children' 'order' 'simple' 'fine' 'deep' 'direction' 'five'\n",
      " 'roles' 'act' 'four' 'eyes' 'sort' 'sometimes' 'question' 'knows' 'jack'\n",
      " 'supporting' 'coming' 'voice' 'heart' 'truly' 'save' 'jokes' 'computer'\n",
      " 'boring' 'level' 'killer' 'strong' 'stop' 'jackie' 'guys' 'room' 'genre'\n",
      " 'body' 'women' 'beginning' 'ends' 'space' 'attempt' 'thriller' 'tom'\n",
      " 'happens' 'fiction' 'note' 'yes' 'says' 'york' 'tells' 'quickly' 'novel'\n",
      " 'hope' 'romantic' 'stupid' 'possible' 'saw' 'peter' 'oscar' 'scream'\n",
      " 'lead' 'career' 'murder' 'hero' 'child' 'extremely' 'manages' 'lee'\n",
      " 'mostly' 'wonder' 'ship' 'particularly' 'dog' 'future' 'sound' 'worse'\n",
      " 'piece' 'fans' 'involving' 'appears' 'involved' 'mean' 'none' 'taking'\n",
      " 'brother' 'police' 'sets' 'attention' 'laugh' 'eventually' 'single'\n",
      " 'falls' 'emotional' 'late' 'hours' 'material' 'power' 'lack' 'fall' 'van'\n",
      " 'result' 'elements' 'planet' 'meet' 'science' 'bring' 'wild' 'living'\n",
      " 'hell' 'husband' 'experience' \"what's\" 'interest' 'theater' 'paul'\n",
      " 'leads' \"movie's\" 'word' 'feature' 'battle' 'obviously' 'alone' 'within'\n",
      " 'god' 'usually' 'enjoy' 'guess' 'among' 'feeling' 'laughs' 'taken' 'king'\n",
      " 'george' 'talk' 'chance' 'disney' 'talent' 'middle' 'easy' 'across'\n",
      " 'number' 'needs' 'attempts' 'happen' 'deal' 'poor' 'form' 'release'\n",
      " 'killed' 'forced' 'aliens' 'girls' 'whether' 'wonderful' 'feels' 'tale'\n",
      " 'serious' 'expect' 'except' 'light' 'success' 'features' 'premise'\n",
      " 'happy' 'television' 'leave' 'chris' 'important' 'meets' 'history'\n",
      " 'giving' 'type' 'words' 'call' 'turned' 'released' 'art' 'impressive'\n",
      " 'working' 'seemed' 'score' 'told' 'girlfriend' 'recent' 'mission'\n",
      " 'basically' 'entertainment' 'apparently' 'easily' 'crew' 'stuff' 'change'\n",
      " 'crime' 'office' 'surprise' 'cool' 'parts' 'somehow' 'parents' 'robin'\n",
      " 'cut' 'credits' 'brings' 'sequel' 'cop' 'suspense' 'events' 'die'\n",
      " 'reality' 'local' 'talking' 'difficult' 'using' 'went' 'writing'\n",
      " 'remember' 'near' 'ago' 'certain' 'hilarious' 'slow' 'blood' 'straight'\n",
      " 'ryan' 'complete' 'popular' 'effective' 'mystery' 'fast' 'william' 'due'\n",
      " 'runs' 'flick' 'gone' 'return' 'red' 'quality' 'smith' 'viewer'\n",
      " 'dramatic' 'ben' 'age' 'business' 'presence' 'batman' 'sexual' 'present'\n",
      " 'surprisingly' 'anyway' 'uses' 'filmmakers' 'personal' 'figure' 'ways'\n",
      " 'decides' 'begin' 'smart' 'somewhat' 'annoying' 'shots' 'rich' 'previous'\n",
      " 'rock' 'successful' 'minute' 'similar' 'absolutely' 'motion' 'former'\n",
      " 'strange' 'came' 'follow' 'read' 'million' 'starring' 'clear' 'williams'\n",
      " 'familiar' 'kid' 'romance' 'intelligent' 'third' 'project' 'excellent'\n",
      " 'budget' 'jones' 'latest' 'party' 'towards' 'predictable' 'powerful'\n",
      " 'water' 'means' 'law' 'beyond' 'visual' 'amazing' 'leaves' 'secret'\n",
      " 'following' 'jim' 'eye' 'prison' 'animated' 'low' 'actress' 'filled'\n",
      " 'leaving' 'questions' 'nature' 'message' 'box' 'villain' 'definitely'\n",
      " 'add' 'large' 'moving' 'clever' 'create' 'felt' 'usual' 'stories'\n",
      " 'brilliant' 'situation' 'break' 'opens' 'scary' 'cinema' 'ones' 'drug'\n",
      " 'bunch' 'thinking' 'solid' 'giant' 'doubt' 'effect' 'bill' 'learn' 'move'\n",
      " 'company' 'potential' 'seriously' 'wars' 'follows' 'saying' 'country'\n",
      " 'brothers' 'bob' 'huge' 'plan' 'america' 'agent' 'class' 'force'\n",
      " 'created' 'unlike' 'pay' 'non' 'married' 'sweet' 'perfectly' 'general'\n",
      " 'realize' 'mark' 'took' 'decent' 'likely' 'dream' 'view' 'subject'\n",
      " 'understand' 'martin' 'happened' 'enjoyable' 'immediately' 'open'\n",
      " 'points' 'heard' 'sam' 'private' 'stay' 'viewers' 'fails' 'cold'\n",
      " 'overall' 'impossible' 'audiences' 'merely' 'mess' 'free' 'bruce'\n",
      " 'wanted' 'gun' 'appear' 'exciting' 'chase' 'escape' 'ultimately' 'ten'\n",
      " 'neither' 'fan' 'inside' 'scott' 'favorite' 'modern' 'wedding' 'brought'\n",
      " 'richard' 'trouble' 'otherwise' 'liked' 'tim' 'trek' 'dumb' 'murphy'\n",
      " 'studio' 'musical' 'political' 'various' 'talented' 'particular' 'stand'\n",
      " 'harry' 'keeps' 'english' 'silly' 'state' 'situations' 'rating'\n",
      " 'slightly' 'steve' 'truth' 'teen' 'bond' 'joke' 'spend' 'frank' 'biggest'\n",
      " 'society' 'air' 'effort' 'focus' 'element' 'earlier' 'members' 'purpose'\n",
      " 'showing' 'soundtrack' 'memorable' 'six' 'hands' 'cannot' 'offers'\n",
      " 'government' 'key' 'rated' 'heavy' 'totally' 'park' 'control' 'credit'\n",
      " 'west' 'ideas' 'wait' 'sit' 'eddie' 'female' 'ask' 'waste' 'terrible'\n",
      " 'depth' 'sci' 'aspect' 'list' 'animation' 'entirely' 'moves' 'actual'\n",
      " 'british' 'constantly' 'fire' 'convincing' 'gave' 'tension' 'brief'\n",
      " 'ridiculous' 'fear' 'cinematography' 'typical' 'ability' 'carter'\n",
      " 'setting' 'spent' 'street' 'quick' 'violent' 'screenwriter' 'subtle'\n",
      " 'expected' 'fairly' 'killing' 'army' 'tone' 'lots' 'thinks' 'cheap'\n",
      " 'suddenly' 'atmosphere' 'background' 'sister' 'highly' 'mars' 'sees'\n",
      " 'complex' 'disaster' 'beauty' 'front' 'seven' 'indeed' 'flat' 'wrote'\n",
      " 'whatever' 'dull' 'humans' 'recently' 'hate' 'steven' 'outside' 'wish'\n",
      " 'greatest' 'master' 'impact' 'amusing' 'sounds' 'telling' 'minor' 'sight'\n",
      " 'cinematic' 'stone' 'hold' 'mary' 'meanwhile' 'cute' 'awful' 'clearly'\n",
      " 'song' 'plenty' 'theme' 'hear' 'college' 'amount' 'truman' 'dreams'\n",
      " 'shown' 'double' 'queen' 'reasons' 'miss' 'allen' 'woody' 'longer'\n",
      " 'approach' \"let's\" 'common' 'max' 'island' 'believable' 'charm' \"year's\"\n",
      " 'godzilla' 'club' 'chemistry' 'possibly' 'casting' 'nick' 'realistic'\n",
      " 'cameron' 'french' 'trailer' 'produced' 'imagine' 'choice' 'ride'\n",
      " 'titanic' 'somewhere' 'road' 'sean' 'leader' 'carry' 'tough' 'thin'\n",
      " 'slowly' 'delivers' 'provide' 'puts' 'asks' 'appearance' 'famous'\n",
      " 'member' 'okay' 'race' 'energy' 'sent' 'detective' 'development' 'etc'\n",
      " 'language' 'proves' 'hot' 'seemingly' 'intelligence' 'caught' 'decide'\n",
      " 'opportunity' 'incredibly' 'brown' 'images' 'blue' 'basic' 'knew'\n",
      " 'interested' 'considering' 'thanks' 'remains' 'climax' 'event' 'critics'\n",
      " 'directing' 'mike' 'leading' 'ground' 'lies' 'forget' 'alive' 'baby'\n",
      " 'vampire' 'conclusion' 'provides' 'trip' 'writers' 'central' 'pace'\n",
      " 'willis' 'grace']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 데이터 준비\n",
    "reivews = [  movie_reviews.raw(fileid) for fileid in movie_reviews.fileids() ]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(vocabulary=word_features)\n",
    "reviews_cv = cv.fit_transform(reivews)\n",
    "print(f'특성수 : {cv.get_feature_names_out()}')\n",
    "reviews_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9006f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
