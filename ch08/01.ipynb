{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d92387",
   "metadata": {},
   "source": [
    "### ìì—°ì–´ ê°ì„±ë¶„ì„\n",
    "- ê°ì„±ì‚¬ì „ ê¸°ë°˜ : ë¯¸ë¦¬ì •ì˜ëœ ê°ì„± ë‹¨ì–´ ì‚¬ì „ ì‚¬ìš©(ê·œì¹™ ê¸°ë°˜)\n",
    "    - TextBlob, AFINNm VADER\n",
    "- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ : ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ í•™ìŠµ(í†µê³„ ê¸°ë°˜)\n",
    "    - TF-IDF ë²¡í„°í™”\n",
    "    - ì„ í˜•íšŒê·€\n",
    "    - ë¡œì§€ìŠ¤í‹±íšŒê·€\n",
    "    - F1 Score, Recison, Recall  -> classification report\n",
    "### ì‚¬ìš© ë°ì´í„°\n",
    "- NLTK ì˜í™” ë¦¬ë·°(2000ê°œ)\n",
    "- ë‹¤ìŒì˜í™”ë¦¬ë·°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c10d0c",
   "metadata": {},
   "source": [
    "#### ì•Œê³ ë¦¬ì¦˜\n",
    " - TextBlob     ì‚¬ì „ê¸°ë°˜ ê°ì„±ë¶„ì„\n",
    " - AFINN        ê°ì • ì ìˆ˜ ë§¤í•‘\n",
    " - VADER(Valence Aware Dictionary)  ì†Œì…œë¯¸ë””ì–´ ìµœì í™” ê°ì„±ë¶„ì„\n",
    " - TF-IDF       í…ìŠ¤íŠ¸ ë²¡í„°í™”\n",
    " - Multinomial Naive Bayes          í™•ë¥  ê¸°ë°˜ ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c1beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob\n",
    "# ì´ ì˜í™”ëŠ” ì •ë§ ì¢‹ê³  ì¬ë¯¸ìˆë‹¤\n",
    "    # ì¢‹ë‹¤ +1(ê¸ì •)\n",
    "    # ì¬ë¯¸ìˆë‹¤ +1(ê¸ì •)\n",
    "    # +2 > 0 --> ê¸ì •(pos) ë¶„ë¥˜\n",
    "# Polarity(ê·¹ì„±ë„)  (ê¸ì •ë‹¨ì–´ìˆ˜ ê°œìˆ˜ - ë¶€ì •ë‹¨ì–´ ê°œìˆ˜) / ì „ì²´ ë‹¨ì–´ ê°œìˆ˜\n",
    "# -1.0 ~ +1.0\n",
    "# 0 ì¤‘ë¦½\n",
    "# Subjectivity(ì£¼ê´€ì„±) í‰ê°€ëŒ€ìƒ ë‹¨ì–´ ë¹„ìœ¨\n",
    "# 0.0 ~ 1.0\n",
    "# 0 : ê°ê´€ì    1 : ì£¼ê´€ì \n",
    "\n",
    "# ë¬¸ë§¥ë¬´ì‹œí•˜ê³  ë‹¨ì–´ ê·¹ì„±ë§Œ ê³ ë ¤\n",
    "# ì´ ì˜í™”ëŠ” ë‚˜ì˜ì§€ ì•Šë‹¤  -> ë‚˜ì˜ë‹¤(-)  ì•Šë‹¤(-) ë¡œ ì¸ì‹\n",
    "# ë‹¨ì  : ë¹ ë¥¸ ì†ë„, í•™ìŠµ ë¶ˆí•„ìš”\n",
    "# ì‚¬ìš© : ì‹¤ì‹œê°„ ê°ì„±ë¶„ì„,  ìŠ¤íŠ¸ë¦¬ë°ë°ì´í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515bcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"TextBlob is amazingly simple to use.\"), Sentence(\"What a wonderful library for NLP!\")]\n",
      "['TextBlob', 'is', 'amazingly', 'simple', 'to', 'use', 'What', 'a', 'wonderful', 'library', 'for', 'NLP']\n",
      "[('TextBlob', 'NNP'), ('is', 'VBZ'), ('amazingly', 'RB'), ('simple', 'JJ'), ('to', 'TO'), ('use', 'VB'), ('What', 'WP'), ('a', 'DT'), ('wonderful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('NLP', 'NNP')]\n",
      "['textblob', 'wonderful library', 'nlp']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "polarity : 0.5\n",
      "subjectivity : 0.6785714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "text = \"TextBlob is amazingly simple to use. What a wonderful library for NLP!\"\n",
    "blob =  TextBlob(text)\n",
    "print(blob.sentences)\n",
    "print(blob.words)\n",
    "print(blob.tags)\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "print(blob.noun_phrases)\n",
    "print('-'*100)\n",
    "# ê°ì„±ë¶„ì„\n",
    "blob.sentiment\n",
    "print(f'polarity : {blob.sentiment.polarity}')\n",
    "print(f'subjectivity : {blob.sentiment.subjectivity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFINN(Lexicon-Based)  ê°ì„±ì‚¬ì „\n",
    "# ê° ë‹¨ì–´ì˜ -5 ~ +5ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  í•©ì‚°\n",
    "# ì´ ì˜í™”ëŠ” ì¢‹ì§€ë§Œ ì¢‹ì§€ ì•Šì€ ë¶€ë¶„ë„ ìˆë‹¤\n",
    "    # ì¢‹ë‹¤ +3  ì¢‹ë‹¤ +3  ë‚˜ì˜ë‹¤ -3  =  +3 > 0 ê¸ì •\n",
    "# score = sum(word_sentiment_value)\n",
    "# ë¶„ë¥˜ê·œì¹™ score > 0 ê¸ì •  score < 0 ë¶€ì •  \n",
    "# ì´ëª¨í‹°ì½˜ì§€ì›\n",
    "# ê°•ë„í‘œí˜„ ì¸ì‹ very , really ë“±\n",
    "\n",
    "# ê°•ì¡° ìˆ˜ì •ì(intensifiers)\n",
    "    # ë§¤ìš°ì¢‹ë‹¤ = 1.5X(ì¢‹ë‹¤ì˜ ì ìˆ˜)\n",
    "\n",
    "# AFINN vs TextBlob\n",
    "# AFINN : ë” ì •í™•í•œ ì ìˆ˜ ë§¤í•‘\n",
    "# TextBlob : ë” ì¼ë°˜ì ì¸ ì ‘ê·¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e5c1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "af = Afinn()\n",
    "text1 = \"TextBlob is amazingly simple to use\"\n",
    "text2 = \"What a wonderful library for NLP!\"\n",
    "score1 = af.score(text1)\n",
    "score2 = af.score(text2)\n",
    "score1, score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER  ì‡¼ì„¤ ë¯¸ë””ì–´ í…ìŠ¤íŠ¸ì— ìµœì í™”\n",
    "# ì´ ì˜í™”ëŠ” ì •ë§ì •ë§ í›Œë¥­í•´!!!\n",
    "# í›Œë¥­í•˜ë‹¤ (ê¸°ë³¸) + 0.7   ì •ë§ì •ë§ (ê°•ì¡°)x1.5\n",
    "# !!! (ë¬¸ì¥ë¶€í˜¸ê°•ì¡°) x1.2\n",
    "# 4ê°œì˜ ê°ì • ì§€ìˆ˜\n",
    "    # positive ê¸ì • í™•ë¥  0 ~ 1\n",
    "    # nagative ë¶€ì • í™•ë¥ \n",
    "    # neutral ì¤‘ë¦½ í™•ë¥ \n",
    "    # compound ì¢…í•©ì ìˆ˜ -1 ~ 1\n",
    "# score = compound_score / sqrt(compound_score**2 + 0.0625)\n",
    "# score >=0.05 ê¸ì •\n",
    "# score <=-0.05 ë¶€ì •  \n",
    "# ê·¸ ì‚¬ì´ëŠ” ì¤‘ë¦½\n",
    "# ëŒ€ì†Œë¬¸ì êµ¬ë¶„  AMAZING amazing ë‹¤ë¥¸ ì ìˆ˜\n",
    "# :) ê¸ì •    :-(  ë¶€ì •\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65a54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adceed6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì¥ : I love this product! It's absolutely amazing ğŸ˜\n",
      "ì ìˆ˜ : {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.862}\n",
      "ë¬¸ì¥ : This is the worst movie I've ever seen...\n",
      "ì ìˆ˜ : {'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n",
      "ë¬¸ì¥ : The food was okay, not great but not bad either.\n",
      "ì ìˆ˜ : {'neg': 0.149, 'neu': 0.487, 'pos': 0.364, 'compound': 0.4728}\n",
      "ë¬¸ì¥ : Iâ€™m REALLY happy with the results!!!\n",
      "ì ìˆ˜ : {'neg': 0.0, 'neu': 0.472, 'pos': 0.528, 'compound': 0.7651}\n",
      "ë¬¸ì¥ : Not good at all. Iâ€™m disappointed.\n",
      "ì ìˆ˜ : {'neg': 0.579, 'neu': 0.421, 'pos': 0.0, 'compound': -0.6711}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyer = SentimentIntensityAnalyzer()\n",
    "sentences = [\n",
    "    \"I love this product! It's absolutely amazing ğŸ˜\",\n",
    "    \"This is the worst movie I've ever seen...\",\n",
    "    \"The food was okay, not great but not bad either.\",\n",
    "    \"Iâ€™m REALLY happy with the results!!!\",\n",
    "    \"Not good at all. Iâ€™m disappointed.\",\n",
    "]\n",
    "for s in sentences:\n",
    "    scores = analyer.polarity_scores(s)\n",
    "    print(f'ë¬¸ì¥ : {s}')\n",
    "    print(f'ì ìˆ˜ : {scores}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from textblob import TextBlob\n",
    "from afinn import Afinn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# nltk ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# ì˜í™” ë¦¬ë·° ë°ì´í„° ë¡œë“œ\n",
    "fileids = movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a601141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = [movie_reviews.raw(fileid) for fileid in  fileids[:50]] + \\\n",
    "        [movie_reviews.raw(fileid) for fileid in  fileids[-50:]]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in  fileids[:50]] + \\\n",
    "        [movie_reviews.categories(fileid)[0] for fileid in  fileids[-50:]]\n",
    "len(reviews), categories.count('pos'), categories.count('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2730af27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •í™•ë„ : 0.6\n"
     ]
    }
   ],
   "source": [
    "# 1 TextBlob\n",
    "def sentiment_textblob(docs):\n",
    "    return ['pos' if TextBlob(doc).sentiment.polarity>0 else 'neg' for doc in docs ]\n",
    "predictions_textblob = sentiment_textblob(reviews)\n",
    "accuracy_textblob = accuracy_score(categories,predictions_textblob)\n",
    "print(f'ì •í™•ë„ : {accuracy_textblob:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89461d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •í™•ë„ : 0.7\n"
     ]
    }
   ],
   "source": [
    "# 2  AFINN\n",
    "def sentiment_afinn(docs):\n",
    "    afn = Afinn(emoticons=True)\n",
    "    return [ 'pos' if afn.score(doc) > 0 else 'neg'  for doc in docs]\n",
    "predictions = sentiment_afinn(reviews)\n",
    "accuracy = accuracy_score(categories,predictions)\n",
    "print(f'ì •í™•ë„ : {accuracy:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14e895b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •í™•ë„ : 0.6\n"
     ]
    }
   ],
   "source": [
    "# 3 VADER\n",
    "def sentiment_vader(docs):\n",
    "    analyer = SentimentIntensityAnalyzer()\n",
    "    return [ 'pos' if analyer.polarity_scores(doc)['compound'] > 0 else 'neg'  for doc in docs]\n",
    "predictions = sentiment_vader(reviews)\n",
    "accuracy = accuracy_score(categories,predictions)\n",
    "print(f'ì •í™•ë„ : {accuracy:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5c7876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„±ë¶„ì„ \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ\n",
    "\n",
    "# ë² ì´ì¦ˆ ì •ë¦¬\n",
    "# \"ì¢‹ë‹¤\" ë‹¨ì–´ë¥¼ ë³¸í›„ ì´ ë¦¬ë·°ê°€ ê¸ì •ì¼ í™•ë¥ \n",
    "# p(ê¸ì • | \"ì¢‹ë‹¤\") = p(\"ì¢‹ë‹¤\" | ê¸ì •) x p(ê¸ì •) / p('ì¢‹ë‹¤')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82da8ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„° ë¶„í• \n",
    "dataset = train_test_split(reviews, categories, test_size=0.2,random_state=42, stratify=categories)\n",
    "len(dataset[0]),len(dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3268bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf ë²¡í„°í™”\n",
    "vectorizer =  TfidfVectorizer(max_features=1000)\n",
    "x_train = vectorizer.fit_transform(dataset[0])\n",
    "x_test = vectorizer.transform(dataset[1])\n",
    "\n",
    "y_train = dataset[2]\n",
    "y_test = dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. mnb\n",
    "mnb_clf = MultinomialNB()\n",
    "mnb_clf.fit(x_train,y_train)\n",
    "predict = mnb_clf.predict(x_test)\n",
    "print( classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logisticregression\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
